{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron data set POI identifier\n",
    "<br>\n",
    "\n",
    "\n",
    "<font color='maroon'>\n",
    "by- Shruti Tiwari </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. In this project, you will play detective, and put your new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. To assist you in your detective work, we've combined this data with a hand-generated list of persons of interest in the fraud case, which means individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='blue'>\n",
    "Q.1 Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to analyze the financial and correspondence information about the executives of enron, building a model to predict a person of interest in the fraud. Since the final objective of the people involved in this fraud was to make money, what could give the better clue for the person of interests than the analysis of their financial data. The supervised machine learning algorithms can be used to train and predict the whether a person is POI or not.\n",
    "### Dataset overview\n",
    "Let's begin with exploring the data. This dataset contains the information about 146 executives and their information in 21 features. The total number of poi's are 18. \n",
    "Next, we have to clean the data. To check for the outliers the scatterplot of salary and bonus can be observed. There is an outlier at around 27 M, which belongs to Total. This is apparently the sum of the entries in each column hence is removed. Other than that, entries with the name  \"THE TRAVEL AGENCY IN THE PARK\" and \"LOCKHART EUGENE E\" have been removed. the entry with the name \"THE TRAVEL AGENCY IN THE PARK\" shows payments were made by Enron employees on account of business-related travel to The Travel Agency in the Park. and there were no entries in any features for \"LOCKHART EUGENE E\".\n",
    "\n",
    "\n",
    "\n",
    " Salary vs Bonus scatterplot with outlier and without the outlier\n",
    " \n",
    "display(HTML(\"<table><tr><td><img src='outliers1.png'></td><td><img src='outliers2.png'></td></tr></table>\"))\n",
    "\n",
    "\n",
    "Other outliers are actually the main money makers.\n",
    "<br>Top 5 people with the highest bonus:\n",
    "\n",
    "| Name| Bonus|\n",
    "|-----|------|\n",
    "|LAVORATO JOHN J | 8000000 |\n",
    "|LAY KENNETH L | 7000000 |\n",
    "|SKILLING JEFFREY K | 5600000 |\n",
    "|BELDEN TIMOTHY N| 5249999|\n",
    "|ALLEN PHILLIP K| 4175000|\n",
    "\n",
    "Top 5 people with the highest salary:\n",
    "\n",
    "| Name| Salary|\n",
    "|-----|------|\n",
    "|SKILLING JEFFREY K | 1111258 |\n",
    "| LAY KENNETH L | 1072321 | \n",
    "|FREVERT MARK A | 1060932 |\n",
    "| PICKERING MARK R | 655037 |\n",
    "| WHALLEY LAWRENCE G | 510364 |\n",
    "\n",
    "All the features except 'poi' have NaN values. NaN's in all the numeric data type has been replaced with zero.\n",
    "\n",
    "The table shows the total count of NaNs in different features.\n",
    "\n",
    "\n",
    "|salary|to_msg|def._pay|tot_pay|loan_adv|bonus|res_stk_def|tot_stk_val|shar_rec_poi |  long_term_ince|\n",
    "| --- | --- | --- | --- | --- | --- |----| ----| -----| ----| \n",
    "| 51 | 60 | 107 |21|142 | 64 | 128 |  20 | 60| 80 |\n",
    "\n",
    "|ex._stk_opt|from_msg|oth.|from_poi_to_this|from_this_to_poi|poi|def._inc.|exp.| rest._stk| dir._fee| \n",
    "|-----| ----| ---|----|-----|----|---|---|----|----|\n",
    "| 44 |60 | 53 | 60 |60 | 0 | 97 | 51 |  36 |129|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Q.2 What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature selection, I thought I could remove one of the two or more parameters which are correlated or give similar information. For instance, There are four stock related features: exercised_stock_options, restricted_stock, restricted_stock_deferred, and total_stock_value. My best guess was to keep total_stock_value and remove other three. Similarly, for from_messages and to_messages one feature could have been removed but which one so I made a guess feature_list based on my intuition but used select k best for feature selection of 10 best features. The reason to take in to account only 10 features was mostly intuitive as I saw many features could be actually correlated to each other. Another way to select number of features can be to put a threshold on the scor obtained by select k best and keep the features above that score :\n",
    "My guess list including two features that I engineered:\n",
    "\n",
    "poi, bonus, salary, total_stock_value, total_payments, deferred_income, long_term_incentive, from_messages, fraction_to_poi_email, shared_receipt_with_poi\n",
    "\n",
    "<br> I used MinMaxScalar transformer from sklearn preprocessing for feature scaling.\n",
    "<br> The transformation is given by:\n",
    "<br>X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "<br>X_scaled = X_std * (max - min) + min\n",
    "\n",
    "I added two features for testing of the model. \n",
    "to_poi_fraction - a fraction of the total 'to' emails that were sent to a POI\n",
    "from_poi_fraction - a fraction of the total 'from' emails that were received from a POI\n",
    "\n",
    "The logic behind adding these features is to include the correspondence between pois. If two persons are involved in a fraud, the ratio of communications between them to their communications with non-pois should be significant. \n",
    "\n",
    "Here is the list of features selected by select k best in score descending order. One of my engineered features fraction_to_poi_email  made on the list. Features selected by select best k is given in the table below with their scores in descending order.\n",
    "\n",
    "| Feature | Score |\n",
    "    | ------ | ----- |\n",
    "    |exercised_stock_options|24.82|\n",
    "    |total_stock_value| 24.18|\n",
    "    |bonus| 20.79| \n",
    "    |salary| 18.29|\n",
    "    |fraction_to_poi_email|16.41|\n",
    "    |deferred_income|11.46|\n",
    "    |long_term_incentive| 9.92|\n",
    "    |restricted_stock | 9.21|\n",
    "    |total_payments|8.77|\n",
    "    |shared_receipt_with_poi| 8.59|\n",
    " \n",
    "It turned out my intuition was not that bad but using select k best made me more confident for proceeding further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Q.3 What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried three algorithms:\n",
    "    1) Decision Tree Classifier\n",
    "    2) Logistic regression\n",
    "    3) Random Forest\n",
    "After evaluation with my 30% testing and 70 % training data, I finally chose Logistic regreesion for validation.\n",
    "\n",
    "All the models did pretty well on the accuracy which is expected and I will discuss it more in answer 6. Logistic regression was the best for recall and Random Forest was the best for precision.\n",
    "The table shows the mean accuracy, mean precision and mean recall for 50 iterations of gridsearch cv for the three algorithms.\n",
    "\n",
    "|             | Decision Tree | Logistic Regression | Random Forest |\n",
    "|------|------||------||------|\n",
    "|  Accuracy  | 0.852 | 0.767 | 0.857 |\n",
    "|Precision| 0.302 | 0.272 | 0.406 |\n",
    "|Recall| 0.2 | 0.6 | 0.16 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Q.4 What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”] \n",
    "</font>\n",
    "\n",
    "Each algorithm has some settings that we can change according to the suitability of our data. Sometimes finding the right settings for those parameters is not that simple. The process of optimizing the parameters of algorithms for best results is called tuning. If we do not do it well, either we do not get the best results from the algorithm or we can be mistaken in our final choice of the algorithm.\n",
    "<br> On the other hand, we do have limitations on adjustment of these parameters. Sometimes the large values of some parameters give us better results, it may also increase the run time of the algorithm. For instance a large n_estimators in random forest classifier ensures a stable metric but it also increases the run time drastically. \n",
    "<br>Therefore it is a good practice, to weigh that the change in the parameter is worth the cost in terms of run time.\n",
    "I used best_params_ and best_estimator_ attribute from GridSearchCV to tune the parameters for all the three algorithms. \n",
    "The parameter grid for three algorithms on which grid search has been performed:\n",
    "<br>\n",
    "\n",
    "|LogisticRegression()|{\"C\": [ 0.5, 1, 10, 10^2, 10^3, ], \"tol\":[10**-1, 10**-4, 10**-5,], \"class_weight\":['balanced']}|\n",
    "| ------- | -------- | --------|\n",
    "|DecisionTreeClassifier()|{\"criterion\": [\"gini\", \"entropy\"], \"min_samples_split\": [10,15,20,25]}|\n",
    "|RandomForestClassifier()|{\"n_estimators\": [25, 50],\"min_samples_split\": [2, 3, 4], \"criterion\": ['gini', 'entropy']}|\n",
    "                    \n",
    "\n",
    "<br>                                    \n",
    "The best parameters for three classifier after performing grid search tuning:\n",
    "<br>\n",
    "Decision tree classifier: (criterion = 'gini', min_samples_split = 10)\n",
    "\n",
    "Logistic regression: (tol = 0.1, C = 1, class_weight = 'balanced')\n",
    "\n",
    "Random forest classifier: (min_samples_split = 3, n_estimators = 25, criterion = 'entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Q.5 What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”] \n",
    "</font>\n",
    "\n",
    "Validation data set is a part of a dataset that we keep separate from training dataset to test the validity of our model. If it is not done properly it could end in a non-homogeneous division i.e. validation dataset or training dataset has too many or too fewer data points for minority label. That can result in a wrong evaluation of model hence wrong predictions.\n",
    "<br> If say we do not validate the data at all i.e. if we train it on all the vailable data, the overfitting may occur. In such cases, the evaluation metric will be high when we are testing the data but on the unseen data the evaluation metric performance may be very poor. That happens because model is only memorizing the data rather than learning to generalize it on unseen data.\n",
    "For the part when I was trying out different algorithms first I used train_test_split function from sklearn.cross_validation and obtained a set of tuned parameters. Then applied it on the test_classifier function of tester.py which incorporates StratifiedShuffleSplit with 1000 folds and did some manual tuning to obtain the optimized evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "Q.6 Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”] </font>\n",
    "\n",
    "  \n",
    "The table below shows the accuracy, precision and recall value that I obtained from my code of evaluate function of tuning.py\n",
    "\n",
    "|             | Decision Tree | Logistic Regression | Random Forest |\n",
    "|------|------||------||------|\n",
    "|  Accuracy  | 0.852 | 0.767 | 0.857 |\n",
    "|Precision| 0.302 | 0.272 | 0.406 |\n",
    "|Recall| 0.2 | 0.6 | 0.16 |\n",
    "\n",
    "A very important part of choosing the right algorithm is to decide the primary evaluation metric for the particular case. It is noticeable that accuracy is pretty high for all three algorithms. The explanation lies in the skewness of data.\n",
    "Accuracy is defined as:\n",
    "\n",
    "Accuracy = (True positive + True negative)/(True positive + False positive + True negative + false negative)\n",
    "\n",
    "Since we have too many negatives in this data, the probability of true negative is high hence the high accuracy.\n",
    "Now to figure out the right metric, I try to answer the question: for this particular case, what is more affordable- too many false negatives or too many false positives.\n",
    "In this case, if I have too many false negatives that means I am missing out on many pois. On the other hand,\n",
    "if I choose too many false positives I am sending a flag to some non-pois for further investigation. \n",
    "Initially I was making the mistake of chhosing only precision as the right choice of evaluation metric. In that scenario I chose random forest algorithm as it gives the best precision value for this case.\n",
    "<br> The project asks for a recall and precision both to be higher than 0.3. With adding new parameters and manually tuning them, the best I could reach was a recall value of 0.29. I therefore started adding parameters and manually tune parameters on Logistic regression classifier.\n",
    "<br>\n",
    "The test_clasifier function which uses StratifiedShuffleSplit gave these metric for \n",
    "Logistic regression with paramer values penalty = 'l1', tol = 0.01, C =0.5, class_weight = 'balanced':\n",
    "<br>\n",
    "<br>\n",
    "<font color = 'blue'> Accuracy: 0.78547\tPrecision: 0.33638\tRecall: 0.62600\tF1: 0.43761\tF2: 0.53404 </font>\n",
    "\n",
    "Interpretation of final metric for enron dataset:\n",
    "The accuracy of o.79 means the model have 79% chances of calling out correct label of poi.\n",
    "<br>Precision = TP/TP+FP\n",
    "<br>Precision of 0.34 mean, this model will identify a positive poi with a probability of 34%.\n",
    "<br> Recall of 0.63 mean, this model will identify a negative poi with a probability of 63%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***********************************************\n",
    "\n",
    "Codes for data exploration starts from here\n",
    "*************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting up required modules and setting path for tools\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "sys.path.append(\"/Users/admin/Desktop/DAND/machine learning/mini_proj/ud120-projects-master/tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "\n",
    "### Loading the data in dictionary \n",
    "with open(\"final_project/final_project_dataset.pkl\", \"r\") as enron_file:\n",
    "    enron_dict = pickle.load(enron_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the data in pandas dataframe\n",
    "enron_data = pd.DataFrame.from_records(list(enron_dict.values()))\n",
    "\n",
    "# setting the keys of dictionary(employees names) as index of enron dataframe\n",
    "employee_names = pd.Series(list(enron_dict.keys()))\n",
    "enron_data.set_index(employee_names, inplace=True)\n",
    "enron_data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no. of POIs and non-POIs \n",
    "poi_count = enron_data.groupby('poi').size()\n",
    "print (poi_count)\n",
    "print \"Total POI's in the present dataset : \",poi_count.iloc[1]\n",
    "print \"Total non-POI's in the present dataset : \",poi_count.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting the outliers\n",
    "import sys\n",
    "sys.path.append(\"/Users/admin/Desktop/DAND/Projects/enron/final_project/tools/\")\n",
    "\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "\n",
    "features = [\"salary\", \"bonus\"]\n",
    "#data_dict.pop('TOTAL', 0)\n",
    "data = featureFormat(enron_dict, features)\n",
    "\n",
    "### plot features\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter( salary, bonus )\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "#plt.show()\n",
    "plt.savefig('outliers1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### remove NAN's from dataset\n",
    "outliers = []\n",
    "\n",
    "for key, value in enron_dict.iteritems():\n",
    "    \n",
    "    val = enron_dict[key]['salary']\n",
    "    if val == 'NaN':\n",
    "        continue\n",
    "    outliers.append((key, int(val)))\n",
    "\n",
    "outliers_final = (sorted(outliers,key=lambda x:x[1],reverse=True)[:5])\n",
    "### print top 4 salaries\n",
    "print(outliers_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enron_dict.pop('TOTAL', 0)\n",
    "features = [\"salary\", \"bonus\"]\n",
    "#data_dict.pop('TOTAL', 0)\n",
    "data = featureFormat(enron_dict, features)\n",
    "\n",
    "### plot features\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    plt.scatter( salary, bonus )\n",
    "\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"bonus\")\n",
    "#plt.show()\n",
    "plt.savefig('outliers2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:udaprogram]",
   "language": "python",
   "name": "conda-env-udaprogram-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
